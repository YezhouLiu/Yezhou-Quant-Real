import sys
import re
from io import StringIO
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

import pandas as pd
import requests
from datetime import date
from utils.config_loader import get_config_value


# ----------------------------------------------------------------------------------------------------------------------------------------
# Ticker éªŒè¯
# ----------------------------------------------------------------------------------------------------------------------------------------
_VALID_TICKER_PATTERN = re.compile(r'^[A-Z]{1,5}([.-][A-Z]{1,2})?$')

def is_valid_us_ticker(ticker: str) -> bool:
    """éªŒè¯æ˜¯å¦ä¸ºåˆæ³•çš„ç¾è‚¡ ticker"""
    if not ticker:
        return False
    return bool(_VALID_TICKER_PATTERN.fullmatch(ticker))


# ----------------------------------------------------------------------------------------------------------------------------------------
# S&P 500 å½“å‰æˆåˆ†ï¼ˆä¿ç•™ç”¨äº Growth Radar éªŒè¯ï¼‰
# ----------------------------------------------------------------------------------------------------------------------------------------
def fetch_sp500_list() -> list[tuple[str, str, str, date]]:
    """æŠ“å– S&P 500 æˆåˆ†è‚¡ï¼ˆä»…ç”¨äºéªŒè¯ï¼Œä¸ä½œä¸ºä¸» universeï¼‰"""
    url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    tables = pd.read_html(url, storage_options=headers)
    df = tables[0]
    tickers = df["Symbol"].astype(str).str.replace(".", "-", regex=False)
    names = df["Security"].astype(str)
    return list(
        zip(tickers, names, ["sp500"] * len(tickers), [date.today()] * len(tickers))
    )


# ----------------------------------------------------------------------------------------------------------------------------------------
# NASDAQ-100 å½“å‰æˆåˆ†ï¼ˆä¿ç•™ç”¨äº Growth Radar - ç§‘æŠ€æˆé•¿è‚¡ä¸“ç”¨ï¼‰
# ----------------------------------------------------------------------------------------------------------------------------------------
def fetch_nasdaq100_list() -> list[tuple[str, str, str, date]]:
    """æŠ“å– NASDAQ-100 æˆåˆ†è‚¡ï¼ˆç”¨äº Growth Radar ç§‘æŠ€è‚¡è¯†åˆ«ï¼‰"""
    url = "https://en.wikipedia.org/wiki/NASDAQ-100"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    tables = pd.read_html(url, storage_options=headers)
    df = None
    for table in tables:
        if "Ticker" in table.columns and "Company" in table.columns:
            df = table
            break
    if df is None:
        raise ValueError("æ‰¾ä¸åˆ°å«æœ‰ NASDAQ-100 è‚¡ç¥¨çš„è¡¨æ ¼")
    tickers = df["Ticker"].astype(str).str.replace(".", "-", regex=False)
    names = df["Company"].astype(str)
    return list(
        zip(tickers, names, ["nasdaq100"] * len(tickers), [date.today()] * len(tickers))
    )


# ----------------------------------------------------------------------------------------------------------------------------------------
# S&P 400 MidCap å½“å‰æˆåˆ†ï¼ˆä¿ç•™ - å®Œæ•´å€™é€‰æ± ï¼‰
# ----------------------------------------------------------------------------------------------------------------------------------------
def fetch_sp400_list() -> list[tuple[str, str, str, date]]:
    """æŠ“å– S&P 400 MidCap æˆåˆ†è‚¡ï¼ˆå®Œæ•´å€™é€‰æ± ï¼Œé»˜è®¤ä¸æ¿€æ´»ï¼‰"""
    url = "https://en.wikipedia.org/wiki/List_of_S%26P_400_companies"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    tables = pd.read_html(url, storage_options=headers)
    df = tables[0]
    
    # çµæ´»æŸ¥æ‰¾ ticker åˆ—ï¼ˆSymbol æˆ– Tickerï¼‰
    ticker_col = 'Symbol' if 'Symbol' in df.columns else 'Ticker' if 'Ticker' in df.columns else df.columns[0]
    # çµæ´»æŸ¥æ‰¾å…¬å¸ååˆ—ï¼ˆSecurity æˆ– Company æˆ– Company Nameï¼‰
    name_col = 'Security' if 'Security' in df.columns else 'Company' if 'Company' in df.columns else 'Company Name' if 'Company Name' in df.columns else df.columns[1]
    
    tickers = df[ticker_col].astype(str).str.replace(".", "-", regex=False)
    names = df[name_col].astype(str)
    return list(
        zip(tickers, names, ["sp400"] * len(tickers), [date.today()] * len(tickers))
    )


# ----------------------------------------------------------------------------------------------------------------------------------------
# S&P 600 SmallCap å½“å‰æˆåˆ†ï¼ˆä¿ç•™ - å®Œæ•´å€™é€‰æ± ï¼‰
# ----------------------------------------------------------------------------------------------------------------------------------------
def fetch_sp600_list() -> list[tuple[str, str, str, date]]:
    """æŠ“å– S&P 600 SmallCap æˆåˆ†è‚¡ï¼ˆå®Œæ•´å€™é€‰æ± ï¼Œé»˜è®¤ä¸æ¿€æ´»ï¼‰"""
    url = "https://en.wikipedia.org/wiki/List_of_S%26P_600_companies"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    tables = pd.read_html(url, storage_options=headers)
    df = tables[0]
    
    # çµæ´»æŸ¥æ‰¾ ticker åˆ—ï¼ˆSymbol æˆ– Tickerï¼‰
    ticker_col = 'Symbol' if 'Symbol' in df.columns else 'Ticker' if 'Ticker' in df.columns else df.columns[0]
    # çµæ´»æŸ¥æ‰¾å…¬å¸ååˆ—ï¼ˆCompany æˆ– Security æˆ– Company Nameï¼‰
    name_col = 'Company' if 'Company' in df.columns else 'Security' if 'Security' in df.columns else 'Company Name' if 'Company Name' in df.columns else df.columns[1]
    
    tickers = df[ticker_col].astype(str).str.replace(".", "-", regex=False)
    names = df[name_col].astype(str)
    return list(
        zip(tickers, names, ["sp600"] * len(tickers), [date.today()] * len(tickers))
    )


# ----------------------------------------------------------------------------------------------------------------------------------------
# âœ… Russell 1000 - ä¸»æˆ˜åœº Universeï¼ˆCore Layerï¼‰
# è¦†ç›–ç¾å›½å¤§ç›˜+ä¸­ç›˜ï¼Œ~90% å¸‚å€¼ï¼Œ~1000 åªè‚¡ç¥¨
# è¿™æ˜¯ PM çº§ç­–ç•¥çš„ä¸»æˆ˜åœºï¼Œä¸é å§”å‘˜ä¼šç­›é€‰ï¼Œåªçœ‹å¸‚å€¼æ’å
# ----------------------------------------------------------------------------------------------------------------------------------------
def fetch_russell1000_list() -> list[tuple[str, str, str, date]]:
    """
    æŠ“å– Russell 1000 æˆåˆ†è‚¡ï¼ˆä¸» Universeï¼‰
    
    æ•°æ®æ¥æºï¼šiShares Russell 1000 ETF (IWB) æŒä»“
    æ³¨æ„ï¼šiShares æä¾›çš„æ˜¯ ETF æŒä»“ï¼Œä¸æŒ‡æ•°æˆåˆ†å¯èƒ½æœ‰å¾®å°å·®å¼‚ï¼Œä½†å¯¹ç ”ç©¶å½±å“å¾ˆå°
    """
    url = "https://www.ishares.com/us/products/239707/ishares-russell-1000-etf/1467271812596.ajax?fileType=csv"
    response = requests.get(url)
    text = response.text
    lines = text.splitlines()
    
    # æ‰¾åˆ°è¡¨å¤´è¡Œ
    for i, line in enumerate(lines):
        if "Ticker" in line or "Symbol" in line:
            header_index = i
            break
    else:
        raise ValueError("âŒ æ‰¾ä¸åˆ°åŒ…å« Ticker çš„è¡¨å¤´è¡Œ")
    
    df = pd.read_csv(StringIO("\n".join(lines[header_index:])))
    
    # è¯†åˆ« ticker å’Œ name åˆ—
    ticker_col = next(
        (c for c in df.columns if c in ["Ticker", "Symbol", "Ticker Symbol"]), None
    )
    name_col = next((c for c in df.columns if "Name" in c or "Company" in c), None)

    if ticker_col is None or name_col is None:
        raise ValueError(
            f"âŒ æœªèƒ½è¯†åˆ« Ticker/Name åˆ—ï¼Œå®é™…åˆ—ä¸º: {', '.join(df.columns)}"
        )
    
    tickers = df[ticker_col].astype(str).str.replace(".", "-", regex=False)
    names = df[name_col].astype(str)
    
    return list(
        zip(
            tickers,
            names,
            ["russell1000"] * len(tickers),
            [date.today()] * len(tickers),
        )
    )


# ----------------------------------------------------------------------------------------------------------------------------------------
# ğŸ” Russell 2000 - Growth Radar åŸæ–™æ± ï¼ˆExploration Layerï¼‰
# ç”¨äºè¯†åˆ«"æ­£åœ¨å˜å¤§çš„é±¼"ï¼Œä½†éœ€è¦è¿‡æ»¤åä½¿ç”¨
# ----------------------------------------------------------------------------------------------------------------------------------------
def fetch_russell2000_list() -> list[tuple[str, str, str, date]]:
    """
    æŠ“å– Russell 2000 æˆåˆ†è‚¡ï¼ˆGrowth Radar åŸæ–™ï¼‰
    
    âš ï¸ ä¸ç›´æ¥ä½œä¸ºä¸» Universeï¼Œè€Œæ˜¯ç”¨äºï¼š
    - ç­›é€‰å¸‚å€¼ Top 20% çš„å°ç›˜çªç ´è‚¡
    - è¯†åˆ«é«˜æˆé•¿æ½œåŠ›æ ‡çš„
    - æ¢ç´¢æ€§ç­–ç•¥ç ”ç©¶
    """
    url = "https://www.ishares.com/us/products/239710/ishares-russell-2000-etf/1467271812596.ajax?fileType=csv"
    response = requests.get(url)
    text = response.text
    lines = text.splitlines()
    
    for i, line in enumerate(lines):
        if "Ticker" in line or "Symbol" in line:
            header_index = i
            break
    else:
        raise ValueError("âŒ æ²¡æ‰¾åˆ° Ticker åˆ—æ‰€åœ¨çš„è¡¨å¤´è¡Œ")
    
    df = pd.read_csv(StringIO("\n".join(lines[header_index:])))
    ticker_col = next(
        (c for c in df.columns if c in ["Ticker", "Symbol", "Ticker Symbol"]), None
    )
    name_col = next((c for c in df.columns if "Name" in c or "Company" in c), None)
    
    if ticker_col is None or name_col is None:
        raise ValueError(
            f"âŒ æœªèƒ½è¯†åˆ« Ticker/Name åˆ—ï¼Œå®é™…åˆ—ä¸º: {', '.join(df.columns)}"
        )
    
    tickers = df[ticker_col].astype(str).str.replace(".", "-", regex=False)
    names = df[name_col].astype(str)
    
    return list(
        zip(tickers, names, ["russell2000"] * len(tickers), [date.today()] * len(tickers))
    )


# ----------------------------------------------------------------------------------------------------------------------------------------
# âœ… ä¿å­˜å€™é€‰è‚¡ç¥¨æ± åˆ° CSVï¼ˆæ”¯æŒæ‰‹åŠ¨ç¼–è¾‘ + ä¿ç•™æ‰‹åŠ¨æ·»åŠ ï¼‰
# 
# æ ¸å¿ƒé€»è¾‘ï¼š
# 1. çˆ¬å–æ‰€æœ‰æ¥æºçš„è‚¡ç¥¨ï¼ˆSP500/400/600 + Russell 1000/2000 + NASDAQ-100ï¼‰
# 2. ä¿ç•™ CSV ä¸­å·²æœ‰çš„æ‰‹åŠ¨æ·»åŠ è‚¡ç¥¨ï¼ˆsource='manual_*'ï¼‰
# 3. å»é‡åä¿å­˜ï¼ˆticker ä¸ºä¸»é”®ï¼‰
# 4. æ”¯æŒæ‰‹åŠ¨ç¼–è¾‘ CSV æ·»åŠ  OpenAI, SpaceX ç­‰å†å²çº§ IPO
# ----------------------------------------------------------------------------------------------------------------------------------------
def save_tradable_candidates_csv(
    csv_path: Path = Path(get_config_value("path.csv_dir")) / "tradable_candidates.csv",
    include_all_sources: bool = True,  # æ˜¯å¦åŒ…å«æ‰€æœ‰æ¥æºï¼ˆSP400/600/Russell2000ï¼‰
    preserve_manual: bool = True,      # æ˜¯å¦ä¿ç•™æ‰‹åŠ¨æ·»åŠ çš„è‚¡ç¥¨
):
    """
    ä¿å­˜å€™é€‰è‚¡ç¥¨æ± åˆ° CSVï¼ˆæ”¯æŒæ‰‹åŠ¨ç¼–è¾‘ï¼‰
    
    Args:
        csv_path: CSV ä¿å­˜è·¯å¾„
        include_all_sources: æ˜¯å¦åŒ…å«æ‰€æœ‰æ¥æºï¼ˆTrue=å®Œæ•´å€™é€‰æ± ï¼ŒFalse=åª Russell 1000ï¼‰
        preserve_manual: æ˜¯å¦ä¿ç•™æ‰‹åŠ¨æ·»åŠ çš„è‚¡ç¥¨ï¼ˆsource='manual_*'ï¼‰
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    - âœ… çˆ¬å–æœ€æ–°æˆåˆ†è‚¡
    - âœ… ä¿ç•™æ‰‹åŠ¨æ·»åŠ çš„è‚¡ç¥¨ï¼ˆOpenAI, SpaceX ç­‰ï¼‰
    - âœ… CSV å¯æ‰‹åŠ¨ç¼–è¾‘ï¼ˆExcel/VSCodeï¼‰
    - âœ… é˜²æ­¢é¢‘ç¹çˆ¬è™«è¢«å°
    
    CSV æ ¼å¼ï¼š
    ticker,company_name,source,added_at,is_active_default,notes
    AAPL,Apple Inc.,russell1000,2024-01-01,FALSE,
    OPENAI,OpenAI Inc.,manual_ipo,2026-01-15,TRUE,Sam Altman AGI company
    """
    print("ğŸ”„ å¼€å§‹æ›´æ–°å€™é€‰è‚¡ç¥¨æ±  CSV...")
    
    # æ­¥éª¤ 1: ä¿ç•™æ‰‹åŠ¨æ·»åŠ çš„è‚¡ç¥¨
    existing_manual = []
    if csv_path.exists() and preserve_manual:
        print("\nğŸ“– è¯»å–ç°æœ‰ CSVï¼Œä¿ç•™æ‰‹åŠ¨æ·»åŠ ...")
        try:
            existing_df = pd.read_csv(csv_path)
            # ä¿ç•™æ‰€æœ‰ source ä»¥ 'manual' å¼€å¤´çš„è®°å½•
            if 'source' in existing_df.columns:
                manual_mask = existing_df['source'].astype(str).str.startswith('manual')
                manual_df = existing_df[manual_mask]
                existing_manual = manual_df.to_dict('records')
                print(f"   âœ… æ‰¾åˆ° {len(existing_manual)} åªæ‰‹åŠ¨æ·»åŠ çš„è‚¡ç¥¨")
        except Exception as e:
            print(f"   âš ï¸  è¯»å–ç°æœ‰ CSV å¤±è´¥: {e}")
    
    # æ­¥éª¤ 2: çˆ¬å–æœ€æ–°æ•°æ®
    print("\nï¿½ çˆ¬å–æœ€æ–°æˆåˆ†è‚¡...")
    
    # æ ¸å¿ƒï¼šRussell 1000ï¼ˆå¿…é¡»ï¼‰
    print("   [Core] Russell 1000...")
    russell1000 = fetch_russell1000_list()
    print(f"      âœ… {len(russell1000)} åªè‚¡ç¥¨")
    
    all_crawled = russell1000
    
    if include_all_sources:
        # å®Œæ•´å€™é€‰æ± ï¼šæ‰€æœ‰æ¥æº
        print("   [Full] S&P 500...")
        sp500 = fetch_sp500_list()
        print(f"      âœ… {len(sp500)} åªè‚¡ç¥¨")
        
        print("   [Full] S&P 400 MidCap...")
        sp400 = fetch_sp400_list()
        print(f"      âœ… {len(sp400)} åªè‚¡ç¥¨")
        
        print("   [Full] S&P 600 SmallCap...")
        sp600 = fetch_sp600_list()
        print(f"      âœ… {len(sp600)} åªè‚¡ç¥¨")
        
        print("   [Full] NASDAQ-100...")
        nasdaq100 = fetch_nasdaq100_list()
        print(f"      âœ… {len(nasdaq100)} åªè‚¡ç¥¨")
        
        print("   [Full] Russell 2000...")
        russell2000 = fetch_russell2000_list()
        print(f"      âœ… {len(russell2000)} åªè‚¡ç¥¨")
        
        all_crawled = sp500 + sp400 + sp600 + russell1000 + nasdaq100 + russell2000
    
    # æ­¥éª¤ 3: åˆå¹¶çˆ¬è™«æ•°æ®å’Œæ‰‹åŠ¨æ•°æ®
    print("\nï¿½ åˆå¹¶æ•°æ®...")
    
    # å°†çˆ¬è™«æ•°æ®è½¬ä¸º DataFrame
    df_crawled = pd.DataFrame(all_crawled, columns=["ticker", "company_name", "source", "added_at"])
    df_crawled['is_active_default'] = False  # çˆ¬è™«æ•°æ®é»˜è®¤ä¸æ¿€æ´»
    df_crawled['notes'] = ''
    
    # å°†æ‰‹åŠ¨æ•°æ®è½¬ä¸º DataFrame
    if existing_manual:
        df_manual = pd.DataFrame(existing_manual)
        # ç¡®ä¿åˆ—ä¸€è‡´
        for col in ['ticker', 'company_name', 'source', 'added_at', 'is_active_default', 'notes']:
            if col not in df_manual.columns:
                df_manual[col] = '' if col == 'notes' else None
        
        # åˆå¹¶
        df_combined = pd.concat([df_crawled, df_manual], ignore_index=True)
    else:
        df_combined = df_crawled
    
    # æ­¥éª¤ 4: è¿‡æ»¤éæ³• ticker
    before_filter = len(df_combined)
    df_combined = df_combined[df_combined['ticker'].apply(is_valid_us_ticker)]
    after_filter = len(df_combined)
    if before_filter > after_filter:
        print(f"\nğŸ§¹ è¿‡æ»¤éæ³• ticker: ç§»é™¤ {before_filter - after_filter} æ¡")
    
    # æ­¥éª¤ 5: å»é‡ï¼ˆticker ä¸ºä¸»é”®ï¼Œä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°ï¼‰
    df_combined = df_combined.drop_duplicates(subset=['ticker'], keep='first')
    
    # æ­¥éª¤ 6: ä¿å­˜
    csv_path.parent.mkdir(parents=True, exist_ok=True)
    df_combined.to_csv(csv_path, index=False)
    print(f"\nâœ… æˆåŠŸå†™å…¥ {len(df_combined)} æ¡æœ‰æ•ˆè®°å½•åˆ°: {csv_path}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    crawled_count = len(df_crawled)
    manual_count = len(existing_manual)
    total_count = len(df_combined)
    
    print(f"\nâœ… å€™é€‰è‚¡ç¥¨æ± å·²æ›´æ–°")
    print(f"   ğŸ“Š çˆ¬è™«æ•°æ®: {crawled_count} åª")
    print(f"   âœï¸  æ‰‹åŠ¨æ·»åŠ : {manual_count} åª")
    print(f"   ğŸ“ˆ å»é‡åæ€»è®¡: {total_count} åª")
    print(f"   ğŸ“‚ ä¿å­˜è‡³: {csv_path}")
    
    print(f"\nğŸ’¡ æç¤ºï¼š")
    print(f"   - å¯æ‰‹åŠ¨ç¼–è¾‘ CSV æ·»åŠ  OpenAI, SpaceX ç­‰å†å²çº§ IPO")
    print(f"   - source='manual_ipo' çš„è‚¡ç¥¨ä¼šåœ¨ä¸‹æ¬¡æ›´æ–°æ—¶ä¿ç•™")
    print(f"   - is_active_default=TRUE çš„è‚¡ç¥¨å¯¼å…¥æ•°æ®åº“æ—¶ä¼šè‡ªåŠ¨æ¿€æ´»")


# ----------------------------------------------------------------------------------------------------------------------------------------
# æµ‹è¯•å…¥å£
# ----------------------------------------------------------------------------------------------------------------------------------------
if __name__ == "__main__":
    # å®Œæ•´ç‰ˆï¼šæ‰€æœ‰æ¥æºï¼ˆæ¨èï¼‰
    save_tradable_candidates_csv(include_all_sources=True)
    
    # å¦‚æœåªæƒ³è¦ Russell 1000ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰ï¼š
    # save_tradable_candidates_csv(include_all_sources=False)
    
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥ï¼š")
    print("   1. æ£€æŸ¥ CSV æ–‡ä»¶ï¼Œç¡®è®¤æ•°æ®æ­£ç¡®")
    print("   2. å¯æ‰‹åŠ¨ç¼–è¾‘ CSV æ·»åŠ  OpenAI, SpaceX ç­‰è‚¡ç¥¨")
    print("   3. è¿è¡Œ python tasks/import_instruments_from_csv.py å¯¼å…¥æ•°æ®åº“")
